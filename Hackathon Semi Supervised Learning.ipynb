{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Time data pull\n",
    "'''\n",
    "\n",
    "# Import data\n",
    "import pandas as pd\n",
    "import os\n",
    "from googletrans import Translator\n",
    "#os.chdir('d:\\\\')\n",
    "comment_list = []\n",
    "with open('LEI_COMMENT.txt') as f:\n",
    "    comment_list=[line.rstrip('\\n') for line in f]\n",
    "with open('LEI_RATING.txt') as f:\n",
    "    rating_list = [line.rstrip('\\n') for line in f]\n",
    "lei_df = pd.DataFrame()\n",
    "lei_df['comment'] = pd.Series(comment_list)\n",
    "lei_df['lei_rating'] = pd.Series(rating_list)\n",
    "translator = Translator()\n",
    "\n",
    "# Translate French to English\n",
    "\n",
    "def f(x):\n",
    "    try:\n",
    "        return translator.translate(x, dest='en').text\n",
    "    except:\n",
    "        return 'translation not available'\n",
    "\n",
    "lei_df['translated_comment'] = lei_df['comment'].apply(f)\n",
    "\n",
    "# Save file for future use\n",
    "lei_df.to_csv('LEI_DF.txt')\n",
    "\n",
    "EXPORT TO C:\\Users\\khatwd2\\Documents\\lei_sow.txt OF DEL MODIFIED BY NOCHARDEL \n",
    "select \n",
    "a.model_score_rank_no\n",
    ",b.rownumber\n",
    " from \n",
    "(select * from EDW.MAP_MODEL_SCORE where model_id = 1214 and efectv_dt = '2017-08-31')a\n",
    "inner join\n",
    "(select comment,cust_id,ROW_NUMBER() OVER () rownumber from  CHERA23.ENTPR_HACK_LEI) b\n",
    "on a.cust_id = b.cust_id\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random\n",
    "import random\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Clean text to remove stop words and lower case everything\n",
    "import re\n",
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_data(input_str):\n",
    "    try:\n",
    "        input_str = re.sub('<[^<]+?>', ' ', input_str)\n",
    "        out = re.sub('[^A-Za-z0-9]+', ' ', input_str.lower())\n",
    "        out = ' '.join([i for i in out.split() if i not in stop_words])\n",
    "        #out = ' '.join([ wordnet_lemmatizer.lemmatize(porter_stemmer.stem(i)) for i in out.split() if i not in stop_words])\n",
    "        return out\n",
    "    except:\n",
    "        return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load previously formed dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "lei_df = pd.read_csv('LEI_DF.txt', encoding ='latin-1',index_col=0)\n",
    "def f1(x):\n",
    "    if x['translated_comment'] =='translation not available':\n",
    "           return x['comment']\n",
    "    else:\n",
    "        return x['translated_comment']\n",
    "\n",
    "lei_df['final_comment']=lei_df.apply(f1, axis=1)\n",
    "\n",
    "# Copy dataframe for model building\n",
    "lei_df_model= lei_df.copy()\n",
    "\n",
    "# Clean data\n",
    "lei_df_model['final_comment_clean']=lei_df_model['final_comment'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load judith's labeled data\n",
    "lei_label_df = pd.read_excel('Comments_Set2.xlsx', index_col = 'Index')\n",
    "\n",
    "# seperate out labeled and unlabeled samples\n",
    "labeled_samples = list(lei_label_df.index.values)\n",
    "unlabeled_samples = [elem for elem in list(lei_df_model.index.values) if elem not in labeled_samples]\n",
    "\n",
    "# generate test set based on unlabeled samples\n",
    "lei_df_model_test = lei_df_model.iloc[unlabeled_samples].copy()\n",
    "\n",
    "#change encoding to UTF-8\n",
    "def encode_f(x):\n",
    "    try:\n",
    "        return x.encode('utf-8')\n",
    "    except:\n",
    "        return 'not available'.encode('utf-8')\n",
    "    \n",
    "lei_df_model['final_comment_clean']=lei_df_model['final_comment_clean'].apply(encode_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the pretrained glove word embeddings and helper functions\n",
    "filename = 'glove.6B\\\\glove.6B.50d.txt'\n",
    "words = pd.read_table(filename, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def vec(w):\n",
    "  return words.loc[w].as_matrix()\n",
    "\n",
    "def get_sent_embeddings(x):\n",
    "    sent_vec = []\n",
    "    for word in x.split():\n",
    "        try:\n",
    "            sent_vec.append(vec(str(word)[2:-1]))\n",
    "        except:\n",
    "            sent_vec.append(vec('unk'))\n",
    "    sent_vec = np.array(sent_vec)\n",
    "    sent_vec =  np.mean(sent_vec, axis=0)\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the comments' word to vector\n",
    "l=lei_df_model['final_comment_clean'].apply(get_sent_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe out of the word embeddings\n",
    "numberOfRows = lei_df_model.shape[0]\n",
    "column_names = []\n",
    "for i in range(50):\n",
    "    column_names.append('vec'+str(i))\n",
    "# create dataframe\n",
    "d = pd.DataFrame(index=np.arange(0, numberOfRows), columns = column_names)\n",
    "\n",
    "# now fill it up row by row\n",
    "for x in np.arange(0, numberOfRows):\n",
    "    #loc or iloc both work here since the index is natural numbers\n",
    "    d.loc[x] = l[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = d.loc[labeled_samples]\n",
    "df_labeled['label_sent'] = lei_label_df['Sentiment']\n",
    "df_labeled['label_td'] = lei_label_df['TD only']\n",
    "df_labeled.dropna(inplace = True)\n",
    "z = {1:1,0:0,-1:2}\n",
    "df_labeled['label_sent']=df_labeled['label_sent'].map(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled['label_td'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = df_labeled.drop(['label_sent','label_td'], axis = 1).as_matrix() \n",
    "#y =  df_labeled['label_sent'].as_matrix() \n",
    "y =  df_labeled['label_td'].as_matrix() \n",
    "\n",
    "# semisupervised learning try 1\n",
    "\n",
    "label_prop_model = LabelPropagation()\n",
    "rng = np.random.RandomState(0)\n",
    "random_unlabeled_points = rng.rand(len(y)) < 0.5\n",
    "labels = np.copy(y)\n",
    "labels[random_unlabeled_points] = -1\n",
    "label_prop_model.fit(X, labels)\n",
    "predictions = label_prop_model.predict(X)\n",
    "plot = skplt.metrics.plot_confusion_matrix(y, predictions, normalize=False)\n",
    "print(pd.Series(predictions).value_counts())\n",
    "\n",
    "# Semi supervised learning try 2\n",
    "label_prop_model = LabelPropagation()\n",
    "rng = np.random.RandomState(0)\n",
    "random_unlabeled_points = rng.rand(len(y)) < 0.3\n",
    "labels = np.copy(y)\n",
    "labels[random_unlabeled_points] = -1\n",
    "label_prop_model.fit(X, labels)\n",
    "predictions = label_prop_model.predict(X)\n",
    "plot = skplt.metrics.plot_confusion_matrix(y, predictions, normalize=False)\n",
    "print(pd.Series(predictions).value_counts())\n",
    "\n",
    "# normal supervised algo using 30% unlabeled points\n",
    "basemodel = OneVsRestClassifier(SVC(kernel='linear',probability=True))\n",
    "predictions = basemodel.fit(X,y).predict(X)\n",
    "plot = skplt.metrics.plot_confusion_matrix(y, predictions, normalize=False)\n",
    "print(pd.Series(predictions).value_counts())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sow_rank=pd.read_csv('lei_sow.txt',  encoding = 'latin-1',error_bad_lines=False, names = ['model_rank','model_sc'],index_col = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sow_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = sow_rank.join(lei_label_df, how='inner')\n",
    "predictions = pd.DataFrame(label_prop_model.predict(X), index = df_labeled.index,columns= ['prediction'])\n",
    "result =result.join(predictions,how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['sow_model_prediction']= result['model_sc'].apply(lambda x: (x>=0.6)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = skplt.metrics.plot_confusion_matrix(result[result['TD only']==1]['sow_model_prediction'], result[result['TD only']==1]['prediction'], normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.scatter(result[result['TD only']==1]['sow_model_prediction'],result[result['TD only']==1]['prediction'])\n",
    "#skplt.metrics.plot_confusion_matrix(result[result['TD only']==1]['sow_model_prediction'],result[result['TD only']==1]['prediction'], normalize=False)\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "with PdfPages('labeled dataset.pdf') as pdf:\n",
    "    # Get current size\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    # Set figure width to 12 and height to 9\n",
    "    fig_size[0] = 12\n",
    "    fig_size[1] = 12\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    \n",
    "    plt.hist([result[result['TD only']==1]['sow_model_prediction'],result[result['TD only']==1]['prediction']],label =['sow model','our model'])\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"TD Only Customers\") \n",
    "    plt.legend()\n",
    "    pdf.savefig()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series(label_prop_model.predict(d.dropna().as_matrix()), index = d.dropna().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result2 =d.dropna().join(sow_rank,how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result2['predictions']= predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result2['sow_model_prediction']= result2['model_sc'].apply(lambda x: (x>=0.6)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = skplt.metrics.plot_confusion_matrix(result2['sow_model_prediction'], result2['predictions'], normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "with PdfPages('Full data set.pdf') as pdf:\n",
    "    # Get current size\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    # Set figure width to 12 and height to 9\n",
    "    fig_size[0] = 12\n",
    "    fig_size[1] = 12\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    \n",
    "    plt.hist([result2['sow_model_prediction'],result2['predictions']],label =['sow model','our model'])\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"Customers\")\n",
    "    plt.legend()\n",
    "    pdf.savefig()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "y_30 = np.copy(y)\n",
    "y_30[rng.rand(len(y)) < 0.3] = -1\n",
    "y_50 = np.copy(y)\n",
    "y_50[rng.rand(len(y)) < 0.5] = -1\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "\n",
    "ls30 = (label_propagation.LabelSpreading().fit(X, y_30),y_30)\n",
    "ls50 = (label_propagation.LabelSpreading().fit(X, y_50),y_50)\n",
    "ls100 = (label_propagation.LabelSpreading().fit(X, y), y)\n",
    "rbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)\n",
    "\n",
    "predictions = ls30.predict(X)\n",
    "plot = skplt.metrics.plot_confusion_matrix(ytrue, predictions, normalize=True)\n",
    "\n",
    "predictions = ls50.predict(X)\n",
    "plot = skplt.metrics.plot_confusion_matrix(ytrue, predictions, normalize=True)\n",
    "\n",
    "predictions = ls100.predict(X)\n",
    "plot = skplt.metrics.plot_confusion_matrix(ytrue, predictions, normalize=True)\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "datasets = [make_moons(noise=0.3, random_state=0)]\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, ytrue = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "\n",
    "# label a few points \n",
    "labeled_N = 240\n",
    "ys = np.array([-1]*len(ytrue)) # -1 denotes unlabeled point\n",
    "random_labeled_points = np.union1d(np.random.choice(np.ravel(np.where(ytrue == -1)), 50), \n",
    "                                   np.random.choice(np.ravel(np.where(ytrue == 1)), 50))\n",
    "random_labeled_points = np.union1d(random_labeled_points,np.random.choice(np.ravel(np.where(ytrue == 0)), 50)) \n",
    "\n",
    "ys[random_labeled_points] = ytrue[random_labeled_points]\n",
    "\n",
    "# Basemodel using Scikit learn\n",
    "basemodel =  OneVsRestClassifier(SVC(kernel='linear',probability=True))\n",
    "#basemodel = SGDClassifier(loss='log', penalty='l1') \n",
    "predictions = basemodel.fit(X[random_labeled_points, :], ys[random_labeled_points]).predict(X)\n",
    "plot = skplt.metrics.plot_confusion_matrix(ytrue, predictions, normalize=True)\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lei_df_model['final_comment_clean'][1200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
